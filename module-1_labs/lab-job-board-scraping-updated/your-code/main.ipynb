{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Job Board Scraping Lab\n",
    "\n",
    "In this lab you will first see a minimal but fully functional code snippet to scrape the LinkedIn Job Search webpage. You will then work on top of the example code and complete several chanllenges.\n",
    "\n",
    "### Some Resources \n",
    "\n",
    "- [Requests library](http://docs.python-requests.org/en/master/#the-user-guide) documentation \n",
    "- [Beautiful Soup Doc](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Urllib](https://docs.python.org/3/library/urllib.html#module-urllib)\n",
    "- [re lib](https://docs.python.org/3/library/re.html)\n",
    "- [Scrapy](https://scrapy.org/)\n",
    "- [List of HTTP status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "- [HTML basics](http://www.simplehtmlguide.com/cheatsheet.php)\n",
    "- [CSS basics](https://www.cssbasics.com/#page_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "\"\"\"\n",
    "This function searches job posts from LinkedIn and converts the results into a dataframe.\n",
    "\"\"\"\n",
    "def scrape_linkedin_job_search(keywords, num_pages=0):\n",
    "    \n",
    "        \n",
    "    # Define the base url to be scraped.\n",
    "    # All uppercase variable name signifies this is a constant and its value should never unchange\n",
    "    BASE_URL = 'https://www.linkedin.com/jobs/search/?'\n",
    "    \n",
    "    # Assemble the full url with parameters\n",
    "    scrape_url = ''.join([BASE_URL, 'keywords=', keywords])\n",
    "    \n",
    "# ---------------------------- adding the num_pages_param ------------------------------------------    \n",
    "    urls = [scrape_url]\n",
    "    if num_pages:\n",
    "        n = 0\n",
    "        for i in range(num_pages):            \n",
    "            urls.append(scrape_url + '&start='+str(int(i) * n))\n",
    "            n += 25                       \n",
    "        \n",
    "    # Create a request to get the data from the server \n",
    "    page = requests.get(scrape_url)\n",
    "    pages = [page]    \n",
    "    \n",
    "    if num_pages:\n",
    "        for i in urls:\n",
    "            pages.append(requests.get(i))\n",
    "    \n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    soups = str(soup)\n",
    "    if num_pages:\n",
    "        for i in pages:\n",
    "            soups += '\\n' + str(BeautifulSoup(i.text, 'html.parser'))\n",
    "        soup = BeautifulSoup(soups, 'html.parser')\n",
    "    \n",
    "    # Create an empty dataframe with the columns consisting of the information you want to capture\n",
    "    columns = ['Title', 'Company', 'Location']\n",
    "    data = pd.DataFrame(columns=columns)\n",
    "\n",
    "    # Retrieve HTML code from the webpage. Parse the HTML into a list of \"cards\".\n",
    "    # Then in each job card, extract the job title, company, and location data.\n",
    "    titles = []\n",
    "    companies = []\n",
    "    locations = []\n",
    "    for card in soup.select(\"div.result-card__contents\"):\n",
    "        title = card.findChild(\"h3\", recursive=False)\n",
    "        company = card.findChild(\"h4\", recursive=False)\n",
    "        location = card.findChild(\"span\", attrs={\"class\": \"job-result-card__location\"}, recursive=True)\n",
    "        titles.append(title.string)\n",
    "        companies.append(company.string)\n",
    "        locations.append(location.string)\n",
    "    \n",
    "    # Inject job titles, companies, and locations into the empty dataframe\n",
    "    zipped = zip(titles, companies, locations)\n",
    "    for z in list(zipped):\n",
    "        data=data.append({'Title' : z[0] , 'Company' : z[1], 'Location': z[2]} , ignore_index=True)\n",
    "    \n",
    "    # Return dataframe\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Director of Data Analysis</td>\n",
       "      <td>Opinion Dynamics</td>\n",
       "      <td>Waltham, Massachusetts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Procurement Executive – Data Analysis (Contract)</td>\n",
       "      <td>Blue Signal Search</td>\n",
       "      <td>Decatur, IL, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Data Scientist – Image Recognition and Analysis</td>\n",
       "      <td>Electric Power Research Institute (EPRI)</td>\n",
       "      <td>Charlotte, North Carolina, United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Senior Analyst, Data and Analysis</td>\n",
       "      <td>Digitas North America</td>\n",
       "      <td>Greater Boston Area</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Associate / Business Analyst (Consulting Data ...</td>\n",
       "      <td>Foresight Associates, LLC</td>\n",
       "      <td>Greater Minneapolis-St. Paul Area</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>Core Data - Data Analyst/ Data Architect</td>\n",
       "      <td>Apple</td>\n",
       "      <td>Cupertino, CA, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Partner's Consulting, Inc.</td>\n",
       "      <td>Philadelphia, Pennsylvania</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Brooksource</td>\n",
       "      <td>Philadelphia, Pennsylvania</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>Manager, Data Architecture and Analysis</td>\n",
       "      <td>Verizon Connect</td>\n",
       "      <td>Atlanta, GA, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>24 Seven LLC</td>\n",
       "      <td>Portland, Oregon</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>125 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Title  \\\n",
       "0                            Director of Data Analysis   \n",
       "1     Procurement Executive – Data Analysis (Contract)   \n",
       "2     Data Scientist – Image Recognition and Analysis    \n",
       "3                    Senior Analyst, Data and Analysis   \n",
       "4    Associate / Business Analyst (Consulting Data ...   \n",
       "..                                                 ...   \n",
       "120           Core Data - Data Analyst/ Data Architect   \n",
       "121                                       Data Analyst   \n",
       "122                                       Data Analyst   \n",
       "123            Manager, Data Architecture and Analysis   \n",
       "124                                       Data Analyst   \n",
       "\n",
       "                                      Company  \\\n",
       "0                            Opinion Dynamics   \n",
       "1                          Blue Signal Search   \n",
       "2    Electric Power Research Institute (EPRI)   \n",
       "3                       Digitas North America   \n",
       "4                   Foresight Associates, LLC   \n",
       "..                                        ...   \n",
       "120                                     Apple   \n",
       "121                Partner's Consulting, Inc.   \n",
       "122                               Brooksource   \n",
       "123                           Verizon Connect   \n",
       "124                              24 Seven LLC   \n",
       "\n",
       "                                     Location  \n",
       "0                      Waltham, Massachusetts  \n",
       "1                             Decatur, IL, US  \n",
       "2    Charlotte, North Carolina, United States  \n",
       "3                         Greater Boston Area  \n",
       "4           Greater Minneapolis-St. Paul Area  \n",
       "..                                        ...  \n",
       "120                         Cupertino, CA, US  \n",
       "121                Philadelphia, Pennsylvania  \n",
       "122                Philadelphia, Pennsylvania  \n",
       "123                           Atlanta, GA, US  \n",
       "124                          Portland, Oregon  \n",
       "\n",
       "[125 rows x 3 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example to call the function\n",
    "\n",
    "results = scrape_linkedin_job_search('data%20analysis', 2)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 1\n",
    "\n",
    "The first challenge for you is to update the `scrape_linkedin_job_search` function by adding a new parameter called `num_pages`. This will allow you to search more than 25 jobs with this function. Suggested steps:\n",
    "\n",
    "1. Go to https://www.linkedin.com/jobs/search/?keywords=data%20analysis in your browser.\n",
    "1. Scroll down the left panel and click the page 2 link. Look at how the URL changes and identify the page offset parameter.\n",
    "1. Add `num_pages` as a new param to the `scrape_linkedin_job_search` function. Update the function code so that it uses a \"for\" loop to retrieve several pages of search results.\n",
    "1. Test your new function by scraping 5 pages of the search results.\n",
    "\n",
    "Hint: Prepare for the case where there are less than 5 pages of search results. Your function should be robust enough to **not** trigger errors. Simply skip making additional searches and return all results if the search already reaches the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# challenge 1 is saved up, \n",
    "\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "\"\"\"\n",
    "This function searches job posts from LinkedIn and converts the results into a dataframe.\n",
    "\"\"\"\n",
    "def scrape_linkedin_job_search(keywords, num_pages=0):\n",
    "    \n",
    "        \n",
    "    # Define the base url to be scraped.\n",
    "    # All uppercase variable name signifies this is a constant and its value should never unchange\n",
    "    BASE_URL = 'https://www.linkedin.com/jobs/search/?'\n",
    "    \n",
    "    # Assemble the full url with parameters\n",
    "    scrape_url = ''.join([BASE_URL, 'keywords=', keywords])\n",
    "    \n",
    "# ---------------------------- adding the num_pages_param ------------------------------------------    \n",
    "    urls = [scrape_url]\n",
    "    if num_pages:\n",
    "        n = 0\n",
    "        for i in range(num_pages):            \n",
    "            urls.append(scrape_url + '&start='+str(int(i) * n))\n",
    "            n += 25                       \n",
    "        \n",
    "    # Create a request to get the data from the server \n",
    "    page = requests.get(scrape_url)\n",
    "    pages = [page]    \n",
    "    \n",
    "    if num_pages:\n",
    "        for i in urls:\n",
    "            pages.append(requests.get(i))\n",
    "    \n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    soups = str(soup)\n",
    "    if num_pages:\n",
    "        for i in pages:\n",
    "            soups += '\\n' + str(BeautifulSoup(i.text, 'html.parser'))\n",
    "        soup = BeautifulSoup(soups, 'html.parser')\n",
    "    \n",
    "    # Create an empty dataframe with the columns consisting of the information you want to capture\n",
    "    columns = ['Title', 'Company', 'Location']\n",
    "    data = pd.DataFrame(columns=columns)\n",
    "\n",
    "    # Retrieve HTML code from the webpage. Parse the HTML into a list of \"cards\".\n",
    "    # Then in each job card, extract the job title, company, and location data.\n",
    "    titles = []\n",
    "    companies = []\n",
    "    locations = []\n",
    "    for card in soup.select(\"div.result-card__contents\"):\n",
    "        title = card.findChild(\"h3\", recursive=False)\n",
    "        company = card.findChild(\"h4\", recursive=False)\n",
    "        location = card.findChild(\"span\", attrs={\"class\": \"job-result-card__location\"}, recursive=True)\n",
    "        titles.append(title.string)\n",
    "        companies.append(company.string)\n",
    "        locations.append(location.string)\n",
    "    \n",
    "    # Inject job titles, companies, and locations into the empty dataframe\n",
    "    zipped = zip(titles, companies, locations)\n",
    "    for z in list(zipped):\n",
    "        data=data.append({'Title' : z[0] , 'Company' : z[1], 'Location': z[2]} , ignore_index=True)\n",
    "    \n",
    "    # Return dataframe\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 2\n",
    "\n",
    "Further improve your function so that it can search jobs in a specific country. Add the 3rd param to your function called `country`. The steps are identical to those in Challange 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "\"\"\"\n",
    "This function searches job posts from LinkedIn and converts the results into a dataframe.\n",
    "\"\"\"\n",
    "def scrape_linkedin_job_search(keywords, num_pages=0 , country=''):\n",
    "    \n",
    "    location = country.strip()\n",
    "        \n",
    "    # Define the base url to be scraped.\n",
    "    # All uppercase variable name signifies this is a constant and its value should never unchange\n",
    "    BASE_URL = 'https://www.linkedin.com/jobs/search/?'\n",
    "    \n",
    "    # Assemble the full url with parameters\n",
    "    scrape_url = ''.join([BASE_URL, 'keywords=', keywords])\n",
    "    if location:\n",
    "            scrape_url = ''.join([BASE_URL, 'keywords=', keywords, '&location=', location] )\n",
    "# ---------------------------- adding the num_pages_param ------------------------------------------    \n",
    "    urls = [scrape_url]\n",
    "    if num_pages:\n",
    "        n = 0\n",
    "        for i in range(num_pages):            \n",
    "            urls.append(scrape_url + '&start='+str(int(i) * n))\n",
    "            n += 25                       \n",
    "        \n",
    "    # Create a request to get the data from the server \n",
    "    page = requests.get(scrape_url)\n",
    "    pages = [page]    \n",
    "    \n",
    "    if num_pages:\n",
    "        for i in urls:\n",
    "            pages.append(requests.get(i))\n",
    "    \n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    soups = str(soup)\n",
    "    if num_pages:\n",
    "        for i in pages:\n",
    "            soups += '\\n' + str(BeautifulSoup(i.text, 'html.parser'))\n",
    "        soup = BeautifulSoup(soups, 'html.parser')\n",
    "    \n",
    "    # Create an empty dataframe with the columns consisting of the information you want to capture\n",
    "    columns = ['Title', 'Company', 'Location']\n",
    "    data = pd.DataFrame(columns=columns)\n",
    "\n",
    "    # Retrieve HTML code from the webpage. Parse the HTML into a list of \"cards\".\n",
    "    # Then in each job card, extract the job title, company, and location data.\n",
    "    titles = []\n",
    "    companies = []\n",
    "    locations = []\n",
    "    for card in soup.select(\"div.result-card__contents\"):\n",
    "        title = card.findChild(\"h3\", recursive=False)\n",
    "        company = card.findChild(\"h4\", recursive=False)\n",
    "        location = card.findChild(\"span\", attrs={\"class\": \"job-result-card__location\"}, recursive=True)\n",
    "        titles.append(title.string)\n",
    "        companies.append(company.string)\n",
    "        locations.append(location.string)\n",
    "    \n",
    "    # Inject job titles, companies, and locations into the empty dataframe\n",
    "    zipped = zip(titles, companies, locations)\n",
    "    for z in list(zipped):\n",
    "        data=data.append({'Title' : z[0] , 'Company' : z[1], 'Location': z[2]} , ignore_index=True)\n",
    "    \n",
    "    # Return dataframe\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Senior Data Analyst</td>\n",
       "      <td>komoot</td>\n",
       "      <td>Berlin, Berlin, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Munich, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Data Analyst - Finance</td>\n",
       "      <td>Jimdo</td>\n",
       "      <td>Hamburg, Hamburg, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Head of Risk Analytics &amp; Data Science (all gen...</td>\n",
       "      <td>Spotcap Global</td>\n",
       "      <td>Berlin und Umgebung, Deutschland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Euro London Appointments</td>\n",
       "      <td>Berlin, Berlin, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>Senior Data Analyst / Consultant</td>\n",
       "      <td>Agoda</td>\n",
       "      <td>Munich, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>Data Scientist - Lead Scoring and Pricing - Be...</td>\n",
       "      <td>Frontier Car Group</td>\n",
       "      <td>Berlin, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>Senior Manager Data Analysis (m/w/d)</td>\n",
       "      <td>human council gmbh</td>\n",
       "      <td>Munich, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>Data-Analyst /Data-Scientist (m/f/d)</td>\n",
       "      <td>Schwäbisch Media</td>\n",
       "      <td>Stuttgart, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>Data Scientist (f/m/d) Computer Vision &amp; AI-su...</td>\n",
       "      <td>Beiersdorf</td>\n",
       "      <td>Hamburg, DE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>125 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Title  \\\n",
       "0                                  Senior Data Analyst   \n",
       "1                                         Data Analyst   \n",
       "2                               Data Analyst - Finance   \n",
       "3    Head of Risk Analytics & Data Science (all gen...   \n",
       "4                                         Data Analyst   \n",
       "..                                                 ...   \n",
       "120                   Senior Data Analyst / Consultant   \n",
       "121  Data Scientist - Lead Scoring and Pricing - Be...   \n",
       "122               Senior Manager Data Analysis (m/w/d)   \n",
       "123               Data-Analyst /Data-Scientist (m/f/d)   \n",
       "124  Data Scientist (f/m/d) Computer Vision & AI-su...   \n",
       "\n",
       "                      Company                          Location  \n",
       "0                      komoot           Berlin, Berlin, Germany  \n",
       "1                      Amazon                        Munich, DE  \n",
       "2                       Jimdo         Hamburg, Hamburg, Germany  \n",
       "3              Spotcap Global  Berlin und Umgebung, Deutschland  \n",
       "4    Euro London Appointments           Berlin, Berlin, Germany  \n",
       "..                        ...                               ...  \n",
       "120                     Agoda                        Munich, DE  \n",
       "121        Frontier Car Group                        Berlin, DE  \n",
       "122        human council gmbh                        Munich, DE  \n",
       "123          Schwäbisch Media                     Stuttgart, DE  \n",
       "124                Beiersdorf                       Hamburg, DE  \n",
       "\n",
       "[125 rows x 3 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = scrape_linkedin_job_search('data%20analysis', 2, 'Germany')\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 3\n",
    "\n",
    "Add the 4th param called `num_days` to your function to allow it to search jobs posted in the past X days. Note that in the LinkedIn job search the searched timespan is specified with the following param:\n",
    "\n",
    "```\n",
    "f_TPR=r259200\n",
    "```\n",
    "\n",
    "The number part in the param value is the number of seconds. 259,200 seconds equal to 3 days. You need to convert `num_days` to number of seconds and supply that info to LinkedIn job search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "\"\"\"\n",
    "This function searches job posts from LinkedIn and converts the results into a dataframe.\n",
    "\"\"\"\n",
    "def scrape_linkedin_job_search(keywords, num_pages=0 , country='', num_days=0):\n",
    "    \n",
    "    # ---------------------- ADDING THE LOCATION PARAM -------------------------------\n",
    "    location = country.strip()\n",
    "        \n",
    "    # Define the base url to be scraped.\n",
    "    # All uppercase variable name signifies this is a constant and its value should never unchange\n",
    "    BASE_URL = 'https://www.linkedin.com/jobs/search/?'\n",
    "    \n",
    "    # Assemble the full url with parameters\n",
    "    scrape_url = ''.join([BASE_URL, 'keywords=', keywords])\n",
    "    if location:\n",
    "            scrape_url = ''.join([BASE_URL, 'keywords=', keywords, '&location=', location] )\n",
    "   \n",
    "# ###### ---------------------- ADDING THE DAY PARAM -------------------------------\n",
    "    location = country.strip() \n",
    "    day = 86400 * int(num_days)\n",
    "    if num_days:\n",
    "            scrape_url = ''.join([BASE_URL,  '?f_TPR=r', str(day), '&geoId=101282230&keywords=', keywords, '&location=', location] )\n",
    "# ---------------------------- adding the num_pages_param ------------------------------------------    \n",
    "    urls = [scrape_url]\n",
    "    if num_pages:\n",
    "        n = 0\n",
    "        for i in range(num_pages):            \n",
    "            urls.append(scrape_url + '&start='+str(int(i) * n))\n",
    "            n += 25                       \n",
    "        \n",
    "    # Create a request to get the data from the server \n",
    "    page = requests.get(scrape_url)\n",
    "    pages = [page]    \n",
    "    \n",
    "    if num_pages:\n",
    "        for i in urls:\n",
    "            pages.append(requests.get(i))\n",
    "    \n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    soups = str(soup)\n",
    "    if num_pages:\n",
    "        for i in pages:\n",
    "            soups += '\\n' + str(BeautifulSoup(i.text, 'html.parser'))\n",
    "        soup = BeautifulSoup(soups, 'html.parser')\n",
    "    \n",
    "    # Create an empty dataframe with the columns consisting of the information you want to capture\n",
    "    columns = ['Title', 'Company', 'Location']\n",
    "    data = pd.DataFrame(columns=columns)\n",
    "\n",
    "    # Retrieve HTML code from the webpage. Parse the HTML into a list of \"cards\".\n",
    "    # Then in each job card, extract the job title, company, and location data.\n",
    "    titles = []\n",
    "    companies = []\n",
    "    locations = []\n",
    "    for card in soup.select(\"div.result-card__contents\"):\n",
    "        title = card.findChild(\"h3\", recursive=False)\n",
    "        company = card.findChild(\"h4\", recursive=False)\n",
    "        location = card.findChild(\"span\", attrs={\"class\": \"job-result-card__location\"}, recursive=True)\n",
    "        titles.append(title.string)\n",
    "        companies.append(company.string)\n",
    "        locations.append(location.string)\n",
    "    \n",
    "    # Inject job titles, companies, and locations into the empty dataframe\n",
    "    zipped = zip(titles, companies, locations)\n",
    "    for z in list(zipped):\n",
    "        data=data.append({'Title' : z[0] , 'Company' : z[1], 'Location': z[2]} , ignore_index=True)\n",
    "    \n",
    "    # Return dataframe\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Professional Consultant (f/m/d) Procurement Da...</td>\n",
       "      <td>CAMELOT Management Consultants</td>\n",
       "      <td>Mannheim, Baden-Württemberg, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Microsoft Analytics Consultant (w/m/x)</td>\n",
       "      <td>Avanade</td>\n",
       "      <td>Düsseldorf, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Product Owner (m/f/d)</td>\n",
       "      <td>Limehome GmbH</td>\n",
       "      <td>München, Bayern, Deutschland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Data Architect - McKinsey Digital</td>\n",
       "      <td>McKinsey &amp; Company</td>\n",
       "      <td>Berlin, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Senior Data Analyst</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Munich, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>Learning &amp; Development Specialist: Data Analys...</td>\n",
       "      <td>HERE Technologies</td>\n",
       "      <td>Berlin, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>Senior Data Scientist - Data &amp; Machine Learnin...</td>\n",
       "      <td>Delivery Hero</td>\n",
       "      <td>Berlin, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>BI &amp; Data Analytics Projekt-Manager</td>\n",
       "      <td>Airbus</td>\n",
       "      <td>Donauwörth, Bayern, Deutschland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>Business Data Analyst</td>\n",
       "      <td>zeroG</td>\n",
       "      <td>Raunheim, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Lab1886</td>\n",
       "      <td>Berlin Area, Germany</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>125 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Title  \\\n",
       "0    Professional Consultant (f/m/d) Procurement Da...   \n",
       "1               Microsoft Analytics Consultant (w/m/x)   \n",
       "2                                Product Owner (m/f/d)   \n",
       "3                    Data Architect - McKinsey Digital   \n",
       "4                                  Senior Data Analyst   \n",
       "..                                                 ...   \n",
       "120  Learning & Development Specialist: Data Analys...   \n",
       "121  Senior Data Scientist - Data & Machine Learnin...   \n",
       "122                BI & Data Analytics Projekt-Manager   \n",
       "123                              Business Data Analyst   \n",
       "124                              Senior Data Scientist   \n",
       "\n",
       "                            Company                              Location  \n",
       "0    CAMELOT Management Consultants  Mannheim, Baden-Württemberg, Germany  \n",
       "1                           Avanade                        Düsseldorf, DE  \n",
       "2                     Limehome GmbH          München, Bayern, Deutschland  \n",
       "3                McKinsey & Company                            Berlin, DE  \n",
       "4                            Amazon                            Munich, DE  \n",
       "..                              ...                                   ...  \n",
       "120               HERE Technologies                            Berlin, DE  \n",
       "121                   Delivery Hero                            Berlin, DE  \n",
       "122                          Airbus       Donauwörth, Bayern, Deutschland  \n",
       "123                           zeroG                          Raunheim, DE  \n",
       "124                         Lab1886                  Berlin Area, Germany  \n",
       "\n",
       "[125 rows x 3 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = scrape_linkedin_job_search('data analysis', 2, 'Germany', 24)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Challenge\n",
    "\n",
    "Allow your function to also retrieve the \"Seniority Level\" of each job searched. Note that the Seniority Level info is not in the initial search results. You need to make a separate search request for each job card based on the `currentJobId` value which you can extract from the job card HTML.\n",
    "\n",
    "After you obtain the Seniority Level info, update the function and add it to a new column of the returned dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
